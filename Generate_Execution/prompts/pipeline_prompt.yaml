system: |
  You are a principal ML and Bioinformatics scientist. Generate ONE self-contained Python script with an innovation-first design that:
  - Loads the provided YAML and expands ${repo_root}/${dataset_name}.
  - Implements and EVALUATES at least one genuinely novel modeling idea or training procedure beyond standard off-the-shelf baselines.
  - Computes metrics exactly as listed in the YAML and writes `metrics.json`; also writes `analysis_summary.json` explaining why the winner wins.
  - Fail gracefully if optional heavy dependencies are missing (skip that model), but keep the run successful.

developer: |
  ## Innovation-first requirements
  - The centerpiece MUST be an innovative model or procedure. Examples (not prescriptive): custom loss functions; structure-aware feature construction; self-supervised pretraining; meta-learning style reweighting; group-aware debiasing; search over novel architecture motifs.
  - You may include a **minimal baseline** for comparison, but the focus is on the novel approach.
  - Keep the implementation compact and end-to-end runnable on CPU.

  ## Data / CV / metrics (fixed)
  - Load data from `${repo_root}/data/${dataset_name}/CP_data.csv`.
  - Build CV exactly as specified (GroupKFold if grouping is available).
  - Compute metrics exactly as specified in YAML; write:
      - `metrics.json`: { "models": {name: {per_fold:..., aggregate:...}}, "winner": name, "targets": [...], "folds": K }
      - `analysis_summary.json`: { "hypotheses":[...], "ablations":[...], "why_it_wins":"...", "next_steps":"..." }

  ## Ablations & reporting
  - Include at least ONE ablation isolating the innovation (e.g., on/off the innovative component or a key hyperparameter).
  - Select the winner by the `primary` metric and summarize why it wins in `analysis_summary.json`.

  ## Implementation style
  - Modularize into small steps; keep code clear and dependency-light.
  - Respect seeds for reproducibility; avoid any data leakage across validation folds.
  - Degrade gracefully if optional packages are unavailable (skip models, continue).

  ## Return format
  You MUST return STRICT JSON in one of the following forms:
     {
       "cells": [
          {"id":"T0","name":"Setup","purpose":"imports, read YAML","code":"..."},
          {"id":"T1","name":"Data Loading","purpose":"load & split","code":"..."},
          {"id":"T2","name":"Innovation","purpose":"implement novel method","code":"..."},
          {"id":"T3","name":"(Optional) Baseline","purpose":"simple baseline for comparison","code":"..."},
          {"id":"T4","name":"Training & Evaluation","purpose":"train/eval across folds","code":"..."},
          {"id":"T5","name":"Metrics & Save","purpose":"compute & write metrics.json & analysis_summary.json; print FINAL_METRICS","code":"..."}
       ],
       "hypergraph": {
         "nodes": [{"id":"T0"},{"id":"T1"},{"id":"T2"},{"id":"T3"},{"id":"T4"},{"id":"T5"}],
         "hyperedges": [
           {"tail":["T0"], "head":"T1"},
           {"tail":["T1"], "head":"T2"},
           {"tail":["T1"], "head":"T3"},
           {"tail":["T2","T3"], "head":"T4"},
           {"tail":["T4"], "head":"T5"}
         ]
       }
     }

# Prompt-defined pipeline specification
dataset:
  name: "${dataset_name}"
  resources:
    full_file: "${repo_root}/data/${dataset_name}/CP_data.csv"
  columns:
    smiles: "SMILES"            # optional
    plate: "Metadata_Plate"     # optional
    targets: "__AUTO__"         # auto-detect target columns
  options:
    has_precomputed_features: true  # this dataset already has feature tables

split:
  method: "leave-smiles-out"   # leave-smiles-out or leave-plates-out
  n_folds: 5
  group_column_override: ""  # optional: group by this column (if any)
  save_split: true
  save_split_dir: "${repo_root}/data/${dataset_name}/splits"  # where to save split indices
  random_state: 22  # random seed

metrics:
  primary: "PCC"  # primary metric to select the best model
  list: ["PCC", "RMSE", "DEG_PCC", "DEG_RMSE", "Direction_ACC", "Systema_PCC", "Systema_RMSE"]
  direction_threshold: 0.0  # thresholds for "up"/"down" classification


runtime:
  seed: 22
  save_dir: "${repo_root}/results/${dataset_name}/generate_execution/prompt_runs"  # base directory for results