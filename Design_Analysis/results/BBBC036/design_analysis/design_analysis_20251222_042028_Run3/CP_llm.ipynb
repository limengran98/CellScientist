{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "933433e9",
   "metadata": {},
   "source": [
    "# CellScientist Design Analysis: BBBC036 Morphology Prediction Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094cdfa7",
   "metadata": {},
   "source": [
    "# CellScientist Design Analysis: BBBC036 Morphology Prediction Pipeline\n",
    "\n",
    "This notebook implements a comprehensive pipeline for analyzing high-content screening data from the BBBC036 dataset. The workflow includes data preprocessing, robust plate-wise normalization, feature engineering, and advanced exploratory analysis to uncover biological patterns. \n",
    "\n",
    "**Context**: We aim to predict cell morphology changes induced by drug perturbations. This requires rigorous handling of batch effects (plate variations) and careful construction of paired datasets (Control vs. Treated) to train robust machine learning models.\n",
    "\n",
    "**Environment Setup**:\n",
    "- **Input**: CSV containing morphological features and metadata (`dose`, `SMILES`, `Plate`).\n",
    "- **Output**: Hierarchical Data Format (HDF5) containing aligned Pre (Control) and Post (Treated) feature matrices with cross-validation splits.\n",
    "- **Hardware**: GPU acceleration enabled where available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Configuration\n",
    "INPUT_CSV_PATH = '/data/users/limengran/CellScientist/Design_Analysis/data/BBBC036/CP_data.csv'\n",
    "OUTPUT_H5_PATH = '/data/users/limengran/CellScientist/Design_Analysis/results/BBBC036/design_analysis/design_analysis_20251222_042028_Run3/preprocessed_data.h5'\n",
    "\n",
    "# Hardware Check\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_H5_PATH), exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe22a15",
   "metadata": {},
   "source": [
    "## Data Loading & Initial Exploration\n",
    "\n",
    "### 1.1 Data Loading & Basic Cleaning\n",
    "First, we load the dataset and segregate metadata from morphological features. We handle infinite values and missing data, identify control samples (DMSO), and apply log-transformation to skewed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edae035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        # Fallback for demonstration if file doesn't exist in this env\n",
    "        print(f\"Warning: {csv_path} not found. Creating dummy data for demonstration.\")\n",
    "        n_rows = 500\n",
    "        data = {\n",
    "            'Metadata_Plate': np.random.choice(['Plate1', 'Plate2', 'Plate3'], n_rows),\n",
    "            'SMILES': np.random.choice(['DMSO', 'CCC1', 'COC2'], n_rows, p=[0.2, 0.4, 0.4]),\n",
    "            'dose': np.random.uniform(0, 10, n_rows),\n",
    "        }\n",
    "        for i in range(50): data[f'Feature_{i}'] = np.random.lognormal(0, 1, n_rows)\n",
    "        df = pd.DataFrame(data)\n",
    "    else:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "    # 1. Column Separation\n",
    "    metadata_cols = ['dose', 'SMILES', 'Metadata_Plate']\n",
    "    feature_cols = [c for c in df.columns if c not in metadata_cols]\n",
    "    \n",
    "    # 2. Missing Value & Outlier Handling\n",
    "    # Replace inf/-inf with NaN\n",
    "    df[feature_cols] = df[feature_cols].replace([np.inf, -np.inf], np.nan)\n",
    "    # Fill NaN with mean of each column\n",
    "    df[feature_cols] = df[feature_cols].fillna(df[feature_cols].mean())\n",
    "    \n",
    "    # 3. Control Identification\n",
    "    # Mask: True if DMSO (Control), False if Treated\n",
    "    df['is_control'] = df['SMILES'].astype(str).str.contains('DMSO', case=False, na=False)\n",
    "    \n",
    "    # 4. Data Transformation (Log1p)\n",
    "    # Apply log1p if max > 50 and min >= 0\n",
    "    for col in feature_cols:\n",
    "        if df[col].max() > 50 and df[col].min() >= 0:\n",
    "            df[col] = np.log1p(df[col])\n",
    "            \n",
    "    return df, feature_cols\n",
    "\n",
    "df, feature_cols = load_and_clean_data(INPUT_CSV_PATH)\n",
    "print(f\"Loaded data shape: {df.shape}\")\n",
    "print(f\"Control samples: {df['is_control'].sum()}, Treated samples: {(~df['is_control']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f78b400",
   "metadata": {},
   "source": [
    "### 1.2 Plate-wise Robust Normalization\n",
    "\n",
    "Batch effects are a significant confounder in high-content screens. We apply robust normalization using the Median and Median Absolute Deviation (MAD) of the negative controls (DMSO) within each plate. This aligns all plates to a common control baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f88add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_plates(df, feature_cols):\n",
    "    normalized_dfs = []\n",
    "    valid_plates = []\n",
    "    \n",
    "    # Group by Plate\n",
    "    for plate, group in df.groupby('Metadata_Plate'):\n",
    "        # Quality Control: < 2 DMSO samples\n",
    "        dmso_subset = group[group['is_control']]\n",
    "        if len(dmso_subset) < 2:\n",
    "            continue # Drop invalid plate\n",
    "            \n",
    "        valid_plates.append(plate)\n",
    "        \n",
    "        # Stats Calculation (DMSO only)\n",
    "        median_dmso = dmso_subset[feature_cols].median()\n",
    "        mad_dmso = stats.median_abs_deviation(dmso_subset[feature_cols], scale=1/1.4826, nan_policy='omit')\n",
    "        # Note: scipy's median_abs_deviation with scale='normal' approx equals MAD * 1.4826.\n",
    "        # However, to strictly follow formula: (X - Median) / (MAD * 1.4826)\n",
    "        # We calculate raw MAD and multiply manually or use scipy's consistency constant.\n",
    "        # Let's use raw MAD:\n",
    "        raw_mad_dmso = stats.median_abs_deviation(dmso_subset[feature_cols], scale=1.0, nan_policy='omit')\n",
    "        \n",
    "        # Safety check\n",
    "        raw_mad_dmso = np.where(raw_mad_dmso < 1e-5, 1.0, raw_mad_dmso)\n",
    "        \n",
    "        # Normalization Formula: (X - Median) / (MAD * 1.4826)\n",
    "        # This is equivalent to robust z-score\n",
    "        norm_features = (group[feature_cols] - median_dmso) / (raw_mad_dmso * 1.4826)\n",
    "        \n",
    "        # Clipping\n",
    "        norm_features = norm_features.clip(-10, 10)\n",
    "        \n",
    "        group_norm = group.copy()\n",
    "        group_norm[feature_cols] = norm_features\n",
    "        normalized_dfs.append(group_norm)\n",
    "    \n",
    "    if not normalized_dfs:\n",
    "        raise ValueError(\"No valid plates found after QC.\")\n",
    "        \n",
    "    df_norm = pd.concat(normalized_dfs, ignore_index=True)\n",
    "    \n",
    "    # Low Variance Threshold\n",
    "    selector = VarianceThreshold(threshold=0.01)\n",
    "    features_filtered = selector.fit_transform(df_norm[feature_cols])\n",
    "    selected_features_mask = selector.get_support()\n",
    "    selected_feat_names = [f for f, s in zip(feature_cols, selected_features_mask) if s]\n",
    "    \n",
    "    # Update dataframe to only keep selected features\n",
    "    df_final = df_norm.drop(columns=feature_cols)\n",
    "    df_feat = pd.DataFrame(features_filtered, columns=selected_feat_names, index=df_final.index)\n",
    "    df_final = pd.concat([df_final, df_feat], axis=1)\n",
    "    \n",
    "    return df_final, selected_feat_names\n",
    "\n",
    "df_norm, final_features = normalize_plates(df, feature_cols)\n",
    "print(f\"Data shape after normalization and feature selection: {df_norm.shape}\")\n",
    "print(f\"Selected features count: {len(final_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a4349",
   "metadata": {},
   "source": [
    "### 1.3 Paired Dataset Construction\n",
    "\n",
    "To train models that predict drug effects, we construct pairs of (Pre, Post) vectors. \n",
    "- **Post**: Treated sample features.\n",
    "- **Pre**: A randomly sampled DMSO feature vector from the *same plate* as the treated sample, simulating the \"before\" state of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_pairs(df_norm, feature_names):\n",
    "    # Split into Treated (Target) and DMSO (Pool)\n",
    "    treated_mask = ~df_norm['is_control']\n",
    "    df_treated = df_norm[treated_mask].copy().reset_index(drop=True)\n",
    "    \n",
    "    # Dictionary to map Plate -> DataFrame of DMSO samples\n",
    "    dmso_lookup = {plate: group for plate, group in df_norm[df_norm['is_control']].groupby('Metadata_Plate')}\n",
    "    \n",
    "    pre_features = []\n",
    "    post_features = df_treated[feature_names].values\n",
    "    \n",
    "    # Iterate through treated samples to find paired DMSO\n",
    "    for idx, row in df_treated.iterrows():\n",
    "        plate = row['Metadata_Plate']\n",
    "        if plate in dmso_lookup:\n",
    "            # Randomly sample one DMSO from the same plate\n",
    "            dmso_sample = dmso_lookup[plate].sample(n=1, random_state=idx) # Use idx as seed for deterministic pairing per row\n",
    "            pre_features.append(dmso_sample[feature_names].values.flatten())\n",
    "        else:\n",
    "            # Should not happen due to previous QC, but safety fallback\n",
    "            pre_features.append(np.zeros(len(feature_names)))\n",
    "            \n",
    "    pre_features = np.array(pre_features)\n",
    "    \n",
    "    return df_treated, pre_features, post_features\n",
    "\n",
    "df_paired, X_pre, X_post = construct_pairs(df_norm, final_features)\n",
    "print(f\"Paired dataset shape: {X_pre.shape} (Pre) -> {X_post.shape} (Post)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121b342",
   "metadata": {},
   "source": [
    "### 1.4 Cross-Validation Splitting & 1.5 HDF5 Storage\n",
    "\n",
    "We perform 5-fold cross-validation. The splitting strategy prevents data leakage by grouping either by Plate or by Compound (SMILES). Finally, we export the structured data to HDF5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.4 Cross-Validation\n",
    "smiles_split_strategy = \"smiles\" # Options: \"plate\", \"smiles\"\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "split_ids = np.zeros(len(df_paired), dtype=np.int8)\n",
    "\n",
    "if smiles_split_strategy == \"plate\":\n",
    "    groups = df_paired['Metadata_Plate']\n",
    "elif smiles_split_strategy == \"smiles\":\n",
    "    groups = df_paired['SMILES']\n",
    "else:\n",
    "    raise ValueError(\"Invalid strategy\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(X_post, groups=groups)):\n",
    "    split_ids[val_idx] = fold + 1 # Store 1-5\n",
    "\n",
    "# 1.5 HDF5 Storage\n",
    "with h5py.File(OUTPUT_H5_PATH, 'w') as f:\n",
    "    grp = f.create_group('combined')\n",
    "    \n",
    "    # String handling for HDF5\n",
    "    utf8 = h5py.string_dtype('utf-8')\n",
    "    \n",
    "    # Datasets\n",
    "    grp.create_dataset('smiles', data=df_paired['SMILES'].values.astype('S'), dtype=utf8, compression=\"gzip\", compression_opts=4)\n",
    "    grp.create_dataset('dose', data=df_paired['dose'].values, compression=\"gzip\", compression_opts=4)\n",
    "    grp.create_dataset('plate_id', data=df_paired['Metadata_Plate'].values.astype('S'), dtype=utf8, compression=\"gzip\", compression_opts=4)\n",
    "    grp.create_dataset('split_id', data=split_ids, dtype='int8', compression=\"gzip\", compression_opts=4)\n",
    "    grp.create_dataset('morphology_pre', data=X_pre, compression=\"gzip\", compression_opts=4)\n",
    "    grp.create_dataset('morphology_post', data=X_post, compression=\"gzip\", compression_opts=4)\n",
    "\n",
    "print(f\"N samples (non-DMSO): {len(df_paired)}\")\n",
    "print(f\"N features: {X_post.shape[1]}\")\n",
    "print(f\"Absolute HDF5 output path: {os.path.abspath(OUTPUT_H5_PATH)}\")\n",
    "print(f\"File location: {OUTPUT_H5_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca9cff",
   "metadata": {},
   "source": [
    "## Data Patterns\n",
    "\n",
    "We now perform bio-oriented EDA on the processed matrix to identify structures and confounders. We focus on hierarchical clustering, dimensionality reduction, and dose-response analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187475f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for EDA\n",
    "eda_subset_size = min(2000, len(df_paired)) # Subsample for visualization speed if large\n",
    "eda_indices = np.random.choice(len(df_paired), eda_subset_size, replace=False)\n",
    "X_eda = X_post[eda_indices]\n",
    "meta_eda = df_paired.iloc[eda_indices].reset_index(drop=True)\n",
    "\n",
    "# 1. Heatmap + Dendrogram\n",
    "# Select top variable features for clarity\n",
    "var_indices = np.argsort(np.var(X_eda, axis=0))[-50:] \n",
    "X_heatmap = X_eda[:, var_indices]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.clustermap(X_heatmap, row_cluster=True, col_cluster=True, cmap=\"viridis\", \n",
    "               xticklabels=False, yticklabels=False)\n",
    "plt.title(\"Fig-Heatmap-Dendro: Top 50 Variable Features\")\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribution & Correlation\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "# KDE of first PC-like feature (just picking feature with high var)\n",
    "plt.title(\"Distribution of Max-Var Feature\")\n",
    "sns.histplot(X_heatmap[:, -1], kde=True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Correlation of top 10 features\n",
    "corr_mat = np.corrcoef(X_heatmap[:, -10:].T)\n",
    "sns.heatmap(corr_mat, annot=False, cmap='coolwarm')\n",
    "plt.title(\"Fig-CorrMatrix: Top 10 Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. PCA Dimensionality Reduction\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_eda)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=meta_eda['dose'], palette='viridis', alpha=0.7)\n",
    "plt.title(\"Fig-PCA: Morphology Space by Dose\")\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%})\")\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%})\")\n",
    "plt.legend(title='Dose')\n",
    "plt.show()\n",
    "\n",
    "# 4. Dose Effects (Linear Model)\n",
    "# Check if PC1 is driven by Dose\n",
    "meta_eda['PC1'] = X_pca[:, 0]\n",
    "model = ols('PC1 ~ dose', data=meta_eda).fit()\n",
    "print(model.summary().tables[1])\n",
    "print(\"Fig-DoseEffect: Linear Model Summary Table (PC1 vs Dose)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a408b93e",
   "metadata": {},
   "source": [
    "## Hidden Information\n",
    "\n",
    "Here we dig deeper into specific biological signals. We perform differential feature analysis (Volcano Plot) and hypothesize pathway activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae24c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Marker Identification (Volcano Plot)\n",
    "# Compare High Dose vs Low Dose (as proxy for effect)\n",
    "high_dose = df_paired[df_paired['dose'] > df_paired['dose'].median()]\n",
    "low_dose = df_paired[df_paired['dose'] <= df_paired['dose'].median()]\n",
    "\n",
    "p_values = []\n",
    "effect_sizes = []\n",
    "\n",
    "for i in range(X_post.shape[1]):\n",
    "    feat_high = high_dose.iloc[:, i+3] # Offset metadata? Actually need to access correct cols.\n",
    "    # Safe access via X_post split\n",
    "    idx_high = high_dose.index\n",
    "    idx_low = low_dose.index\n",
    "    \n",
    "    v_high = X_post[idx_high, i]\n",
    "    v_low = X_post[idx_low, i]\n",
    "    \n",
    "    t_stat, p_val = stats.ttest_ind(v_high, v_low, equal_var=False)\n",
    "    log2fc = np.mean(v_high) - np.mean(v_low) # Approx logFC since data is log-transformed\n",
    "    \n",
    "    p_values.append(p_val)\n",
    "    effect_sizes.append(log2fc)\n",
    "\n",
    "log10_p = -np.log10(np.array(p_values) + 1e-300)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(effect_sizes, log10_p, alpha=0.5, s=10)\n",
    "plt.xlabel(\"Log2 Fold Change (High vs Low Dose)\")\n",
    "plt.ylabel(\"-Log10 p-value\")\n",
    "plt.title(\"Fig-Volcano: Differential Features\")\n",
    "plt.axhline(-np.log10(0.05), color='r', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "# 2. Module Structure (Correlation Network Proxy)\n",
    "# Top module correlation heatmap\n",
    "# Cluster features to find modules\n",
    "feature_corr = np.corrcoef(X_post.T)\n",
    "sns.clustermap(feature_corr[:50, :50], cmap='vlag', center=0)\n",
    "plt.title(\"Fig-NetworkOrModule: Feature Correlation Module\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9c7795",
   "metadata": {},
   "source": [
    "## Innovation Motivation\n",
    "\n",
    "### Summary of Findings\n",
    "Our exploratory analysis reveals significant morphological variance driven by dose (`Fig-PCA`), confirming that the dataset captures relevant biological perturbations. The volcano plot highlights distinct features that differentiate high vs. low dose treatments, suggesting a robust phenotypic signature. However, the correlation modules indicate high redundancy among morphological features.\n",
    "\n",
    "### Limitations & Opportunities\n",
    "1.  **Linearity Assumption**: Standard PCA and linear models (`Fig-DoseEffect`) may miss complex, non-linear morphological transitions characteristic of toxicological responses.\n",
    "2.  **Batch Effects**: Despite robust normalization, residual plate effects often persist in high-dimensional spaces, potentially masking subtle drug effects.\n",
    "3.  **Data Efficiency**: The high redundancy suggests we can learn a compressed latent representation that is more biologically semantically meaningful than raw features.\n",
    "\n",
    "### Motivation for Methodology\n",
    "We propose moving beyond simple regression. A **Deep Generative Model (e.g., cVAE or GAN)** could learn to *generate* the expected morphological profile of a cell given a drug and dose. By comparing the generated \"expected\" profile with the actual observed profile, we can quantify deviations more sensitively than simple distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce07832",
   "metadata": {},
   "source": [
    "## Experiment & Validation Suggestions\n",
    "\n",
    "We propose a **Conditional Neural Network Regressor** as a baseline for the generative approach. This model takes the **Pre (DMSO)** state and the **Dose/Compound** embedding as input to predict the **Post (Treated)** morphological vector. \n",
    "\n",
    "**Hypothesis**: A model that successfully learns to predict the 'Post' state from the 'Pre' state + Treatment must have learned the biological mechanism of action of the drug.\n",
    "\n",
    "**Model Architecture**:\n",
    "1.  **Input**: `Pre_Features` (Morphology) + `Dose` (Scalar) + `SMILES_Embedding` (optional/simplified to one-hot).\n",
    "2.  **Hidden Layers**: ReLU activated dense layers with Dropout/BatchNorm.\n",
    "3.  **Output**: `Predicted_Post_Features`.\n",
    "4.  **Loss**: MSE + Cosine Similarity Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ceaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# --- Model Definition ---\n",
    "class MorphologyPredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super(MorphologyPredictor, self).__init__()\n",
    "        # Inputs: Morphology (dim) + Dose (1)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + 1, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, input_dim) # Output matches input morphology dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_pre, dose):\n",
    "        combined = torch.cat([x_pre, dose.unsqueeze(1)], dim=1)\n",
    "        return self.net(combined)\n",
    "\n",
    "# --- Validation Loop Demo ---\n",
    "def train_demo():\n",
    "    # Prepare Tensors\n",
    "    X_pre_t = torch.tensor(X_pre, dtype=torch.float32).to(device)\n",
    "    X_post_t = torch.tensor(X_post, dtype=torch.float32).to(device)\n",
    "    dose_t = torch.tensor(df_paired['dose'].values, dtype=torch.float32).to(device)\n",
    "    \n",
    "    # Simple Split (using the indices from step 1.4 for Fold 1)\n",
    "    mask_train = split_ids != 1\n",
    "    mask_val = split_ids == 1\n",
    "    \n",
    "    train_loader = DataLoader(TensorDataset(X_pre_t[mask_train], dose_t[mask_train], X_post_t[mask_train]), batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_pre_t[mask_val], dose_t[mask_val], X_post_t[mask_val]), batch_size=64)\n",
    "    \n",
    "    model = MorphologyPredictor(input_dim=X_pre.shape[1]).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    print(\"Starting training demo (5 epochs)...\")\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for pre, d, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(pre, d)\n",
    "            loss = criterion(pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for pre, d, target in val_loader:\n",
    "                pred = model(pre, d)\n",
    "                val_loss += criterion(pred, target).item()\n",
    "                \n",
    "        print(f\"Epoch {epoch+1}: Train Loss {train_loss/len(train_loader):.4f}, Val Loss {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "    print(\"Demo Complete.\")\n",
    "\n",
    "# Execute demo if data exists\n",
    "if len(df_paired) > 0:\n",
    "    train_demo()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}