{
  "dataset_name": "BBBC036",
  "workflow_phases": [
    "task_analysis"
  ],
  "phases": {
    "task_analysis": {
      "enabled": true,
      "llm_notebook": {
        "split_strategy": "smiles",
        "paths": {
          "data": "/data/users/limengran/CellScientist/Design_Analysis/data/BBBC036/CP_data.csv",
          "paper": "",
          "preprocess": "",
          "out": "/data/users/limengran/CellScientist/Design_Analysis/results/BBBC036/design_analysis/design_analysis_20251222_041833_Run2/CP_llm.ipynb",
          "out_exec": "/data/users/limengran/CellScientist/Design_Analysis/results/BBBC036/design_analysis/design_analysis_20251222_041833_Run2/CP_llm_executed.ipynb",
          "h5_out": "/data/users/limengran/CellScientist/Design_Analysis/results/BBBC036/design_analysis/design_analysis_20251222_041833_Run2/preprocessed_data.h5"
        },
        "exec": {
          "timeout_seconds": 1800,
          "max_preview_rows": 8,
          "max_preview_cols": 20,
          "pdf_max_pages": 10,
          "force_json_mode": true,
          "save_intermediate": true,
          "allow_errors": false,
          "max_fix_rounds": 3,
          "cuda_device_id": 0
        },
        "llm": {
          "provider": "yizhan",
          "model": "gemini-3-pro-all",
          "base_url": "https://vip.yi-zhan.top/v1",
          "api_key": "sk-bTxDIYUAspAvG0qi18B9Db216a5640D59eE0627333693835",
          "temperature": 0.5,
          "max_tokens": 20480,
          "top_p": 1,
          "frequency_penalty": 0,
          "presence_penalty": 0,
          "seed": 2
        },
        "multi": {
          "num_runs": 4,
          "enabled": true,
          "max_parallel_workers": 1,
          "name_template": "NB{idx:02d}-seed{seed}",
          "seeds": [
            1,
            2,
            3,
            4
          ],
          "prompt_variants": [
            "focus=robust-scaling; note: emphasize dose effects",
            "focus=pca/clustering; note: plate-holdout discussion",
            "focus=feature-network; note: correlation modules",
            "focus=enrichment; note: volcano + markers"
          ],
          "out_dir": "/data/users/limengran/CellScientist/Design_Analysis/results/BBBC036/design_analysis",
          "hypergraph_name": "CellScientist-HyperGraph",
          "node_names": [
            "Data Loading & Initial Exploration",
            "Data Patterns",
            "Hidden Information",
            "Innovation Motivation",
            "Experiment & Validation Suggestions"
          ],
          "review": {
            "comment": "This block now only controls reference selection based on heuristics. LLM review and closed-loop are removed.",
            "reference": {
              "enabled": true,
              "export_dir": "/data/users/limengran/CellScientist/Design_Analysis/results/BBBC036/design_analysis/reference",
              "min_overall": 0.72,
              "export_mode": "best_only",
              "stop_after_export": true,
              "keep_history": true,
              "weights": {
                "scientific": 1.0,
                "novelty": 0.8,
                "reproducibility": 1.2,
                "interpretability": 1.0
              }
            }
          }
        },
        "prompt": "You are now acting as a researcher working at the intersection of computational biology and artificial intelligence. I will provide you with a paper abstract/body text and my curated experimental dataset.",
        "focus_instruction": "focus=pca/clustering; note: plate-holdout discussion"
      }
    }
  },
  "literature": {
    "enabled": false,
    "use_existing_only": false,
    "cache_days": 30,
    "max_papers": 12,
    "max_abstract_chars": 1200,
    "prompt_max_chars": 6000,
    "min_citations": 6,
    "provider": "semantic_scholar",
    "task_keywords": "cell painting perturbation modeling",
    "query": ""
  },
  "paths": {
    "literature_dir": "/data/users/limengran/CellScientist/Design_Analysis/results/BBBC036/literature",
    "literature_knowledge_json": "/data/users/limengran/CellScientist/Design_Analysis/results/BBBC036/literature/domain_knowledge.json"
  },
  "prompts": {
    "autofix": {
      "system_prompt": "You are a senior Python engineer and Jupyter expert.\nGiven Jupyter cell errors, return ONLY a MINIFIED JSON object with key 'edits'.\nEach edit MUST be {\"cell_index\": int, \"source\": str} replacing the WHOLE code cell.\nYou MUST cover ALL indices listed in 'target_cell_indices' (one edit per index). Do not add new cells.\nDo not modify markdown cells. No markdown fences or extra text."
    },
    "review": {
      "system_prompt": "You are acting as an expert PI in computational biology and AI. Your task is to review Jupyter notebooks generated by junior researchers. Evaluate them with rigor, considering correctness, scientific validity, novelty, reproducibility, and interpretability. Provide structured, constructive feedback with scores and actionable suggestions. Use concise academic language suitable for lab notes.\n",
      "critique_template": "Review the following notebook content:\n\n{{CONTENT}}\n\nPerform the following:\n\n1. **Correctness**: Does the notebook run without critical errors? Are the methods implemented correctly?\n2. **Scientific Validity**: Are the statistical tests appropriate and results significant?\n3. **Novelty**: Does the analysis show innovation beyond standard baselines?\n4. **Reproducibility**: Are steps clear and data/code paths reproducible?\n5. **Interpretability**: Are plots labeled, results interpretable, and narrative coherent?\n\nThen:\n- Assign a **0–1 score** for each category.\n- Provide a weighted overall score (use reference.weights if given).\n- Summarize key strengths and weaknesses in 3–5 bullet points.\n- If overall score < threshold, propose concrete revisions (list exact steps).\n- If overall score ≥ threshold, mark as candidate for reference notebook.\n\nOutput must be valid JSON with fields: `scores`, `overall`, `strengths`, `weaknesses`, `decision`, `suggestions`."
    },
    "notebook_generation": {
      "system_prompt": "You are an expert computational biologist and ML engineer.\nReturn ONLY a JSON object describing a runnable Jupyter Notebook with keys:\n- \"title\": string\n- \"cells\": array of {{\"type\": \"markdown\"|\"code\", \"source\": \"string\"}}\n\nContext:\n- Input CSV path: {data_path}\n- Required H5 output path: {h5_output_path}\n\nConstraints:\n- Language: {language_label}.\n- Alternate markdown and code logically; each code cell MUST be preceded by a markdown explanation.\n- Code must be executable offline (no internet).\n- Use GPU if available (via torch.cuda.is_available() check), but keep code compatible with CPU fallback.\n- The external environment has already set 'CUDA_VISIBLE_DEVICES' appropriately.\n- The Notebook must explicitly connect data analysis with the scientific context of the provided paper.\n- Include the following section headings (in order), using them exactly:\n{headings_bulleted}\n\nSection expectations:\n  1. **Data Loading & Initial Exploration**\n    **1.1 Data Loading & Basic Cleaning**\n    * **Input**: Load a CSV dataset and separate metadata columns: [\"dose\", \"SMILES\", \"Metadata_Plate\"] and feature columns (all others).\n    * **Column Separation**:\n        * Identify `dose`, `SMILES`, and `Metadata_Plate` as **Metadata**.\n        * Identify all other numeric columns as **Morphological Features**.\n    * **Control Identification**: Create a mask identifying samples containing \"DMSO\" in the `SMILES` column (case-insensitive) as Negative Controls (Pre), and the rest as Treated (Post).\n    * **Missing Value & Outlier Handling**:\n        * Replace `inf` / `-inf` with `NaN`.\n        * Fill `NaN` values using the mean of each feature column.\n    * **Data Transformation**: Apply a `Log1p` ($log(x+1)$) transformation to feature columns where the maximum value is $>50$ and the minimum value is non-negative.\n\n    **1.2 Plate-wise Robust Normalization**\n    * **Grouping Logic**: Process each `Metadata_Plate` independently.\n    * **Quality Control**: If a plate contains $<2$ DMSO samples, treat it as invalid (set features to 0 or drop).\n    * **Statistics Calculation**: Calculate the Median ($Median_DMSO$) and Median Absolute Deviation ($MAD_DMSO$) using **only the DMSO samples** within that plate.\n        * Safety check: If MAD < 1e^{{-5}}, set it to $1.0$ to avoid division by zero.\n    * **Normalization Formula**: Apply the following to all samples in the plate:\n        X_norm = (X - Median_DMSO)\\(MAD_DMSO \\times 1.4826)\n    * **Clipping & Selection**:\n        * Clip standardized values to the range $[-10, 10]$.\n        * Apply `VarianceThreshold(threshold=0.01)` to remove low-variance features.\n\n    **1.3. Paired Dataset Construction**\n    * **Pairing Logic**:\n        * **Post (Target)**: The standardized features of non-DMSO (Treated) samples.\n        * **Pre (Input)**: For *each* Post sample, randomly sample one DMSO feature vector from the **same plate (Metadata_Plate)** to serve as the paired Pre input.\n    * **Alignment**: Ensure the generated Pre/Post matrices have consistent row counts and strictly aligned metadata (SMILES, dose, etc.).\n\n    **1.4. Cross-Validation Splitting**\n    * **Logic**: You MUST perform 5-fold cross-validation on non-DMSO samples using the strategy defined by the variable **`{split_strategy}`**:\n        * If `{split_strategy}` is `\"plate\"` $\\rightarrow$ use `GroupKFold(n_splits=5)` grouping by `Metadata_Plate`.\n        * If `{split_strategy}` is `\"smiles\"` $\\rightarrow$ use `GroupKFold(n_splits=5)` grouping by `SMILES`.\n    * **Index Storage**: The generated fold indices (values $1-5$) must be stored as a `split_id` array with a shape matching the Post features.\n\n    **5. HDF5 Group Storage (Hierarchical Storage with 'combined')**\n    * **Structure Requirement**: Create a single `.h5` file. **You MUST create a top-level group named `combined`**. All data must be stored directly under this group.\n    * **Compression**: Enable `compression=\"gzip\"` for all datasets.\n    * **Dataset List**:\n        1.  `combined/smiles`: $(N,)$, string/object type.\n        2.  `combined/dose`: $(N,)$, float type.\n        3.  `combined/plate_id`: $(N,)$, string type.\n        4.  `combined/split_id`: $(N,)$, integer type ($1-5$), containing the cross-validation indices generated above.\n        5.  `combined/morphology_pre`: $(N, Features)$, float matrix.\n        6.  `combined/morphology_post`: $(N, Features)$, float matrix.\n\n    Notes:\n    - Save strings using h5py.string_dtype(\"utf-8\")\n    - Save split_id as int8\n    - Use compression=\"gzip\" and compression_opts=4\n\n    Print:\n    - N samples (non-DMSO count)\n    - N features\n    - Absolute HDF5 output path\n    - File location: [PRINT THE H5 OUTPUT PATH HERE]\n\n  2. **Data Patterns**\n      - Build directly on the processed matrix from Step 1.\n      - Perform bio-oriented EDA with **advanced visualizations** (prefer these, with graceful fallbacks):\n      * **Heatmap + clustering dendrogram** of top-variable features across samples/conditions.\n          - Preferred: seaborn.clustermap; Fallback: matplotlib + scipy (linkage + dendrogram).\n      * **Distribution & correlation views**: KDE/violin or histogram for representative features; correlation matrix and top correlated pairs.\n      * **Dimensionality reduction**: PCA (required); optionally UMAP/t-SNE if available. Color by condition/dose/plate; overlay point density if feasible.\n      * **Batch & dose effects**: ANOVA or linear/mixed-effects models (statsmodels) with concise result tables.\n      - Provide biological interpretations of observed trends (e.g., heterogeneity, dose-dependent morphological shifts, plate/batch structure).\n      - Expected figures (names in captions): Fig-Heatmap-Dendro, Fig-PCA, Fig-CorrMatrix, Fig-DoseEffect.\n\n  3. **Hidden Information**\n      - Leverage signals from Step 2 and integrate with the paper context to hypothesize hidden biology.\n      - Use at least two of the following **advanced analyses** (choose what the data supports; keep offline):\n      * **Marker identification** across conditions/groups with effect sizes and multiple-testing notes; visualize a **Volcano Plot** (log2FC vs -log10 p) for top features.\n      * **Functional/Pathway enrichment (reasoned offline)**: if gene IDs/sets are available, perform simple over-representation using provided sets; otherwise provide **mechanistic reasoning** and a ranked feature-set summary. Visualize an **Enrichment Bubble Plot** (bubble size = set size, color = significance or effect).\n      * **Network/module structure**: build a **feature correlation network** (NetworkX optional; fallback: adjacency threshold table + degree histogram) or at minimum a top-module correlation heatmap.\n      * **Phenotype association**: regression/logistic models linking PCs or feature modules to dose/condition; report coefficients, CIs, and calibration notes.\n      * **Model interpretability** (if a baseline is fit): show **feature importance**; if SHAP is not available, fall back to standardized coefficients or permutation importance.\n      - Support hypotheses with appropriate tests (t/Wilcoxon/regression) and report effect sizes + 95% CIs where feasible.\n      - Expected figures (names in captions): Fig-Volcano, Fig-Enrichment, Fig-NetworkOrModule, Fig-Association.\n\n  4. **Innovation Motivation**\n      - Start with a concise Markdown summary of the key findings from Step 2 (Data Patterns) and Step 3 (Hidden Information).\n      * Describe in natural language what the analyses revealed: e.g., distributional skew, hidden clusters, feature correlations, inferred biological mechanisms.\n      * This summary acts as a bridge to motivate the next discussion.\n      - Then, in Markdown, discuss:\n      * Limitations of current methods (mapped to observed failure modes: instability due to outliers, plate leakage, poor dose-response fitting, etc.).\n      * Unresolved questions revealed by the dataset (hidden subgroups, confounders, nonlinear responses).\n      * Opportunities for innovation (methodological or biological).\n      - Explicitly connect these insights back to the data-driven findings.\n      - (Optional) Provide a baseline model in code (with leakage-safe splitting) to illustrate current performance and motivate improvements.\n  \n  5. **Experimental and Validation Recommendations**\n\n      - Building upon the innovative motivation outlined in Step 4, logically extend the concept and closely integrate biological findings with computational model design. \n      Task: Predict the high-content morphology of cells after drug perturbation. Write down the model development approach and code demo; execution is not required.\n\nGeneral rules:\n- Every code cell must be explained by a Markdown cell immediately before it.  \n- Markdown must include clear subheadings (## Data Patterns, ## Hidden Information, etc.).  \n- Keep all code concise and runnable, avoid heavy external dependencies.  \n- The Notebook should demonstrate a flow from **data exploration → hidden insights → research motivation → proposed methods**, grounded in the paper’s context.\n",
      "user_prompt": "You are now acting as a researcher working at the intersection of computational biology and artificial intelligence. I will provide you with a paper abstract/body text and my curated experimental dataset."
    },
    "report": {
      "system_prompt": "\nYou are a senior Cell Painting/Bioinformatics data scientist and research writer.\nYou will receive an INPUT_JSON (containing a notebook summary, an H5 artifact summary, idea.json, reference.json, and a configuration snippet).\n\nYour Task:\n- Generate a reproducible and executable summary report (Markdown), focusing on:\n1) Task objectives and problem definition\n2) Data overview (fields/dimensions/grouping/missing data/batch effects, etc.)\n3) Detailed data analysis conclusions (provide evidence: key statistics, visualization points, control groups/stratification, etc.)\n4) Interpretability and potential confounding factors\n5) Reproducible experiments and validation protocols (provide indicators, controls, randomization seeds, ablation/sensitivity analysis)\n6) Recommendations for follow-up experiments (at least 8, as specific as possible to the steps and expected results)\n7) Risks, biases, and precautions\n8) Reproducibility checklist (files/commands/environmental hints/randomization seeds)\n\nOutput Requirements: \n- You must output a JSON object (JSON ONLY) containing the key: report_md.\n- report_md is a Markdown string using a clear heading hierarchy (# / ## / ###). - Do not fabricate non-existent values; if information is missing, mark it as \"unknown/to be confirmed\" and provide suggestions on how to obtain it."
    },
    "literature": {
      "query_generation": {
        "user_prompt": "Generate 3 specific search queries for the biological dataset '{dataset_name}'.\nThe goal is to find information regarding:\n1. Mechanism of Action (MoA) or biological context.\n2. Existing benchmark results or SOTA performance.\n3. Standard analysis pipelines (e.g., processing steps, recommended metrics).\n\nReturn ONLY a JSON object with a single key 'queries' containing a list of strings.\nExample: {{\"queries\": [\"{dataset_name} mechanism of action\", \"{dataset_name} benchmarks\"]}}\n"
      },
      "synthesis": {
        "system_prompt": "You are a senior scientific research assistant specializing in computational biology.\nYour task is to synthesize raw search results into a concise, high-density context block for a downstream Data Analysis Agent.\n",
        "user_prompt": "Source Material ({source_type}):\n{raw_text}\n\nTask:\nCreate a structured summary titled \"Literature & External Knowledge\".\n\nRequirements:\n1. **Biology**: Briefly describe the cell line, perturbation types, or biological relevance.\n2. **Benchmarks**: Mention any known Accuracy/F1 scores or SOTA methods (e.g., \"ResNet50 achieves 95% accuracy\").\n3. **Methods**: Highlight recommended preprocessing (e.g., \"RobustScaler\", \"Whitening\") or modeling strategies.\n\nConstraint:\n- Keep it under 600 words.\n- Cite sources using the format [Title].\n- Return a JSON object: {{\"summary\": \"...your markdown text...\"}}"
      }
    },
    "notebook_generation+analyze": {
      "system_prompt": "You are an expert computational biologist and ML engineer.\nReturn ONLY a JSON object describing a runnable Jupyter Notebook with keys:\n- \"title\": string\n- \"cells\": array of {{\"type\": \"markdown\"|\"code\", \"source\": \"string\"}}\n\nContext:\n- Input CSV path: {data_path}\n- Required H5 output path: {h5_output_path}\n\nConstraints:\n- Language: {language_label}.\n- Alternate markdown and code logically; each code cell MUST be preceded by a markdown explanation.\n- Code must be executable offline (no internet).\n- Use GPU if available (via torch.cuda.is_available() check), but keep code compatible with CPU fallback.\n- The external environment has already set 'CUDA_VISIBLE_DEVICES' appropriately.\n- The Notebook must explicitly connect data analysis with the scientific context of the provided paper.\n- Include the following section headings (in order), using them exactly:\n{headings_bulleted}\n\nSection expectations:\n1. **Data Loading & Initial Exploration**\n  1.1. Load a CSV dataset and separate metadata columns: [\"dose\", \"SMILES\", \"Metadata_Plate\"] and feature columns (all others).\n  1.2. Identify DMSO vs non-DMSO rows using a case-insensitive search of \"DMSO\" in the SMILES column.\n    - DMSO rows are used only to compute baselines and normalization statistics.\n    - Only non-DMSO rows are included in the final saved arrays (N = number of non-DMSO samples).\n\n  1.3. Convert all feature columns to numeric, coercing invalid values to NaN.\n\n  1.4. Treat as \"invalid\" any value that is:\n    - NaN\n    - +inf / -inf\n    - abs(value) > 1e10\n    These invalid values must be handled as missing values.\n\n  1.5. Drop feature columns where more than 95% of entries are invalid (as defined above).\n\n  1.6. You MUST perform 5-fold cross-validation using **{split_strategy}**:\n    - If `{split_strategy}` is `\"plate\"` → use `GroupKFold(n_splits=5)` grouping by `Metadata_Plate` (on non-DMSO samples only).\n    - If `{split_strategy}` is `\"smiles\"` → use `GroupKFold(n_splits=5)` grouping by `SMILES` (on non-DMSO samples only).\n    - The fold index must be stored as `split_id` in the final HDF5 file (values 0–4).\n\n  1.7. For each fold, you must follow this **exact preprocessing order**:\n\n    1.7.1 Split non-DMSO samples into train and test according to GroupKFold indices.\n\n    1.7.2 On the TRAIN subset only:\n        - Replace any invalid values (NaN / ±inf / abs>1e10) with NaN.\n        - Compute the per-feature median on TRAIN (ignoring NaNs).\n        - Use this TRAIN median to fill missing values (NaN / ±inf / abs>1e10) in BOTH TRAIN and TEST.\n        - After this step, TRAIN and TEST must not contain NaN / inf.\n\n    1.7.3 On the imputed TRAIN and TEST:\n        - For each feature, if the minimum value is ≥ 0 and the maximum value > 50,\n          apply `np.log1p` to that feature in BOTH TRAIN and TEST.\n        - Do NOT use any information from the test fold to define thresholds or statistics.\n\n    1.7.4 Construct the DMSO baseline set:\n        - Identify DMSO rows from TRAIN (using SMILES).\n        - If no DMSO rows exist in TRAIN, fallback to using all TRAIN rows as the baseline set.\n\n    1.7.5 MAD-based normalization:\n        - Compute the per-feature median vector `med` from the DMSO baseline set.\n        - Compute the per-feature MAD vector `mad = median(|x - med|)` from the DMSO baseline set.\n        - For any feature where MAD is 0 or extremely small (e.g. < 1e-5), replace MAD with 1.0 to avoid division by zero.\n        - Use these `med` and `mad` to normalize BOTH TRAIN and TEST:\n              X_scaled = (X - med) / (mad * 1.4826)\n\n    1.7.6 Clamping:\n        - Clamp all values in TRAIN and TEST to [−10.0, 10.0].\n\n    1.7.7 Construction of `morphology_post`:\n        - `morphology_post` must be the clamped, normalized feature matrix for the TEST non-DMSO rows only.\n        - Must be float32 and contain no NaN / inf.\n\n    1.7.8 Construction of `morphology_pre` (must match the reference implementation exactly):\n        - From the **scaled TRAIN matrix BEFORE clamping (`X_scaled`)**, select only the DMSO rows.\n        - Attach their corresponding `Metadata_Plate` as a \"plate\" column.\n        - Group by plate and compute the per-feature median to obtain `plate_baseline`.\n        - Separately compute a `global_fallback` vector as the per-feature median of the baseline set **before scaling normalization** (same vector used to compute `med`).\n        - For each TEST sample, map the corresponding plate baseline by reindexing `plate_baseline` using test plate IDs.\n        - For any plate missing a baseline, replace the row with `global_fallback`.\n        - The result is `morphology_pre`: a mapped baseline matrix aligned to TEST indices (N_test × F).\n        - This baseline must **not** undergo any additional transformation after the mapping step.\n        - Cast `morphology_pre` to float32.\n\n  1.8. Store the final merged results into a single HDF5 file,\n    using gzip compression and fixed dtypes, with **no groups and no prefixes**.\n  Required HDF5 structure at the top level:\n\n      ├─ smiles           : str              – SMILES string\n      ├─ morphology_pre   : float32 (N, F)   – plate-wise DMSO baseline row for each sample\n      ├─ morphology_post  : float32 (N, F)   – scaled morphology values for drug-treated samples\n      ├─ dose             : float32\n      ├─ plate_id         : str\n      └─ split_id         : int8  (values 0,1,2,3,4)\n\n  Notes:\n  - N must equal the number of non-DMSO samples (DMSO is used only to compute normalization baselines and is not included in the output arrays).\n  - Save strings using h5py.string_dtype(\"utf-8\")\n  - Save morphology_pre and morphology_post as float32\n  - Save split_id as int8\n  - Use compression=\"gzip\" and compression_opts=4\n\n  After writing the file, reopen it and assert:\n  - morphology_pre contains no NaN\n  - morphology_post contains no NaN\n\n  Print:\n  - N samples (non-DMSO count)\n  - N features\n  - Absolute HDF5 output path\n  - File location: [PRINT THE H5 OUTPUT PATH HERE]\n\n2. **Data Patterns**\n   - Build directly on the processed matrix from Step 1.\n   - Perform bio-oriented EDA with **advanced visualizations** (prefer these, with graceful fallbacks):\n     * **Heatmap + clustering dendrogram** of top-variable features across samples/conditions.\n       - Preferred: seaborn.clustermap; Fallback: matplotlib + scipy (linkage + dendrogram).\n     * **Distribution & correlation views**: KDE/violin or histogram for representative features; correlation matrix and top correlated pairs.\n     * **Dimensionality reduction**: PCA (required); optionally UMAP/t-SNE if available. Color by condition/dose/plate; overlay point density if feasible.\n     * **Batch & dose effects**: ANOVA or linear/mixed-effects models (statsmodels) with concise result tables.\n   - Provide biological interpretations of observed trends (e.g., heterogeneity, dose-dependent morphological shifts, plate/batch structure).\n   - Expected figures (names in captions): Fig-Heatmap-Dendro, Fig-PCA, Fig-CorrMatrix, Fig-DoseEffect.\n\n3. **Hidden Information**\n   - Leverage signals from Step 2 and integrate with the paper context to hypothesize hidden biology.\n   - Use at least two of the following **advanced analyses** (choose what the data supports; keep offline):\n     * **Marker identification** across conditions/groups with effect sizes and multiple-testing notes; visualize a **Volcano Plot** (log2FC vs -log10 p) for top features.\n     * **Functional/Pathway enrichment (reasoned offline)**: if gene IDs/sets are available, perform simple over-representation using provided sets; otherwise provide **mechanistic reasoning** and a ranked feature-set summary. Visualize an **Enrichment Bubble Plot** (bubble size = set size, color = significance or effect).\n     * **Network/module structure**: build a **feature correlation network** (NetworkX optional; fallback: adjacency threshold table + degree histogram) or at minimum a top-module correlation heatmap.\n     * **Phenotype association**: regression/logistic models linking PCs or feature modules to dose/condition; report coefficients, CIs, and calibration notes.\n     * **Model interpretability** (if a baseline is fit): show **feature importance**; if SHAP is not available, fall back to standardized coefficients or permutation importance.\n   - Support hypotheses with appropriate tests (t/Wilcoxon/regression) and report effect sizes + 95% CIs where feasible.\n   - Expected figures (names in captions): Fig-Volcano, Fig-Enrichment, Fig-NetworkOrModule, Fig-Association.\n\n4. **Innovation Motivation**\n   - Start with a concise Markdown summary of the key findings from Step 2 (Data Patterns) and Step 3 (Hidden Information).\n     * Describe in natural language what the analyses revealed: e.g., distributional skew, hidden clusters, feature correlations, inferred biological mechanisms.\n     * This summary acts as a bridge to motivate the next discussion.\n   - Then, in Markdown, discuss:\n     * Limitations of current methods (mapped to observed failure modes: instability due to outliers, plate leakage, poor dose-response fitting, etc.).\n     * Unresolved questions revealed by the dataset (hidden subgroups, confounders, nonlinear responses).\n     * Opportunities for innovation (methodological or biological).\n   - Explicitly connect these insights back to the data-driven findings.\n   - (Optional) Provide a baseline model in code (with leakage-safe splitting) to illustrate current performance and motivate improvements.\n\n5. **Experiment & Validation Suggestions**\n   - Extend logically from the innovation motivations in Step 4, with a tight integration of biological findings and computational model design.\n    Task: Predicting High-Content Cellular Morphology Following Drug Perturbation\n    Your primary task is to develop a multimodal MLP model that accurately estimates the high-dimensional morphological profiles of cells (e.g., from Cell Painting) following drug treatment.\n\n   - In Markdown, propose next-step experiments that explicitly link:\n     * **Biological discovery → Computational modeling**: e.g., heterogeneity in morphology → clustering + ensemble ML classifiers; nonlinear dose–response → generalized additive models, monotonic regression, or spline-regularized ML/DL.\n     * **Mechanistic priors in modeling**: incorporate pathway/group priors into feature engineering, hierarchical models, or biologically-informed regularization.\n     * **Cross-modal integration**: combine morphology with chemical descriptors/omics using multi-view learning (e.g., CCA, multimodal autoencoders, graph-based fusion).\n     * **Generalization & robustness**: explicitly test whether models trained on one plate/dose/compound generalize to unseen conditions, with leakage-safe splits (plate-holdout, dose-stratified).\n   \n   - Define tasks & success criteria:\n\n    task_definition:\n\n      problem_statement: >\n        Predict cellular morphological responses induced by chemical perturbations.\n        The task is formulated as a multi-output regression problem: given a compound,\n        dose level, and baseline cellular morphology, the goal is to predict the\n        post-treatment morphological profile.\n      inputs:\n        compound_structure (smiles):\n          description: SMILES string representing the administered chemical perturbation.\n          role: Encodes molecular identity; later transformed into a vector representation.\n        dose:\n          description: Drug concentration applied to cells.\n          role: Modulates strength of phenotypic response.\n        morphology_pre:\n          description: Baseline morphological profile of control (untreated) cells.\n          shape: (N, F)\n          role: Provides the cellular state before perturbation.\n      output:\n        morphology_post:\n          description: Morphological profile after compound treatment.\n          shape: (N, F)\n          role: Regression target representing phenotypic response.\n      grouping_variable:\n        plate_id:\n          description: Experimental plate identifier.\n          purpose: Enables plate-level data splitting to evaluate batch robustness.\n      formal_task:\n        type: multi_output_regression\n        mapping: >\n          f(compound_structure, dose, morphology_pre) -> morphology_post\n    evaluation_scenarios:\n      unseen_compounds:\n        description: Test on compounds not present in training.\n        goal: Evaluate structural generalization.\n      unseen_batches:\n        description: Test on new plate_id groups.\n        goal: Evaluate robustness to batch/context variation.\n    evaluation_metrics:\n      global:\n        MSE:\n          description: Mean Squared Error between predicted and observed profiles.\n          direction: lower_is_better\n        PCC:\n          description: Pearson Correlation Coefficient across samples.\n          direction: higher_is_better\n        R2:\n          description: Variance explained by predictions relative to ground truth.\n          direction: higher_is_better\n      differential_features:\n        DEG_RMSE:\n          description: RMSE calculated on Top-K most changed features (captures magnitude precision).\n          direction: lower_is_better\n        DEG_PCC:\n          description: PCC calculated on Top-K most changed features (captures trend accuracy).\n          direction: higher_is_better\n        MSE_DM:\n          description: MSE restricted to features significantly changed after perturbation.\n          direction: lower_is_better\n        PCC_DM:\n          description: PCC computed on differentially modulated features only.\n          direction: higher_is_better\n        R2_DM:\n          description: R² computed on differentially modulated features only.\n          direction: higher_is_better\n\nGeneral rules:\n- Every code cell must be explained by a Markdown cell immediately before it.  \n- Markdown must include clear subheadings (## Data Patterns, ## Hidden Information, etc.).  \n- Keep all code concise and runnable, avoid heavy external dependencies.  \n- The Notebook should demonstrate a flow from **data exploration → hidden insights → research motivation → proposed methods**, grounded in the paper’s context.\n",
      "user_prompt": "You are now acting as a researcher working at the intersection of computational biology and artificial intelligence. I will provide you with a paper abstract/body text and my curated experimental dataset."
    },
    "idea": {
      "system_prompt": "You are an expert computational biologist and AI scientist.\nWe have just completed Phase 1: Data Processing & Analysis for a High-Content Screening (Cell Painting) dataset.\nThe data is now cleaned, robust-scaled, paired (treatment vs control), and saved as HDF5.\n\nYour task is to generate 5-10 innovative experimental ideas (hypotheses or modeling approaches) for Phase 2.\nPhase 2 will involve deep learning or advanced statistical modeling on this HDF5 data.\n\nRequirements:\n1. Ideas must be grounded in the data (morphology features, SMILES structures, dose info).\n2. Leverage your knowledge base (biology, chemistry, mechanism of action).\n3. Suggest specific model architectures (e.g., Graph Neural Networks for SMILES + MLP for morphology, Contrastive Learning, Multi-modal fusion, etc.) or biological questions (e.g., Mechanism of Action prediction, Clustering, Outlier detection).\n4. Return a JSON object with a key 'ideas', containing a list of 5-10 ideas.\n   Each idea must have:\n   - 'title': Short, descriptive title.\n   - 'description': 2-3 sentences describing the approach.\n   - 'reasoning': Why this idea is suitable for THIS specific dataset (SMILES + Morphology).\n   - 'expected_impact': What biological or computational insight this might yield."
    }
  }
}