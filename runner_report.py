#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Final report generation + best-code export.

Refactored out of run_cellscientist.py.

Notable behavior:
- Outputs are saved under: results/${dataset_name}/finall_results/ (or logs_dir/finall_results/ if overridden)
- The final report is generated by an LLM using `llm_report` (preferred) or
  fallback `llm` in pipeline_config.json.
- The LLM client uses a robust OpenAI-compatible HTTP call (requests) with
  retry/backoff.
"""

from __future__ import annotations

import json
import os
import re
import textwrap
import time
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests

try:
    import yaml
except Exception:
    yaml = None

from runner_config import deep_merge, drop_none
from runner_utils import (
    export_notebook_as_py,
    read_head_tail_lines,
    read_text,
    read_text_limited,
    safe_copy,
    safe_read_json,
    project_root,
)


FINAL_REPORT_DIRNAME = "finall_results"  # keep user's requested spelling
FINAL_REPORT_PROMPT_PATH = os.path.join(project_root(), "Final_Report", "prompts", "final_report.yaml")


def redact_secrets(obj: Any) -> Any:
    """Best-effort redact API keys / tokens from configs before sending to LLM."""
    try:
        if isinstance(obj, dict):
            out = {}
            for k, v in obj.items():
                lk = str(k).lower()
                if any(x in lk for x in ["api_key", "apikey", "token", "secret", "password"]):
                    out[k] = "<redacted>"
                else:
                    out[k] = redact_secrets(v)
            return out
        if isinstance(obj, list):
            return [redact_secrets(x) for x in obj]
        if isinstance(obj, str) and (obj.startswith("sk-") or obj.startswith("Bearer ")):
            return "<redacted>"
    except Exception:
        return obj
    return obj


def ensure_final_prompt_file_exists() -> None:
    """Create a default final report prompt file if missing (non-destructive)."""
    p = Path(FINAL_REPORT_PROMPT_PATH)
    if p.exists():
        return
    try:
        p.parent.mkdir(parents=True, exist_ok=True)
        default_yaml = textwrap.dedent(
            """\
            system: |
                You are a senior Research Engineer + Computational Biologist.
                You will read the provided: `summary_report.md` for Phase 1, `experiment_report.md` for Phases 2/3,
                pipeline/phase logs, and `pipeline_summary.json`, and generate a final Markdown analysis report.
                Writing Requirements:
                - Only output the Markdown body; do not output YAML/JSON/explanatory introductions.
                - Must cover and explicitly write out:
                1) Task definition and breakdown (what each sub-stage does, what the inputs/outputs are)
                2) Data path, runtime environment, key engineering parameters (GPU/CPU, CUDA_VISIBLE_DEVICES, hyperparameters, etc.)
                3) Experimental exploration process details: the effect of each run/iteration in each sub-stage (success/failure), triggered fixes, key changes
                4) Iteration reflection: why these changes were made, what was learned from failures, what to do next
                5) Final results: best metrics, best solution description, reproduction steps, key file paths

                Suggested output structure (can be fine-tuned, but must be complete):

                - Summary (1-2 paragraphs)
                - Pipeline overview (Phase 1/2/3)
                - Task definition and data/environment
                - Phase 1: Motivation and thought process summary
                - Phase 2: Generate-Execute-Analyze (iteratively)
                - Phase 3: Optimize-Feedback (iteratively)
                - Best results and code saving instructions
                - Failure Cases and Reflections (if any)
                - Appendix: Critical Path List

            user_template: |
               # Input Materials (Please digest and write the final report; do not copy verbatim)

              ## 0) Meta
              dataset_name: ${dataset_name}
              report_output_dir: ${final_output_dir}

              ## 1) Pipeline Summary (json)
              ```json
              ${pipeline_summary_json}
              ```

              ## 2) Phase configs (redacted)
              ### Phase 1 config
              ```json
              ${phase1_config_json}
              ```

              ### Phase 2 config
              ```json
              ${phase2_config_json}
              ```

              ### Phase 3 config
              ```json
              ${phase3_config_json}
              ```

              ## 3) Phase 1 summary_report.md
              ```markdown
              ${phase1_summary_md}
              ```

              ## 4) Phase 2 best experiment_report.md (if any)
              ```markdown
              ${phase2_report_md}
              ```

              ## 5) Phase 3 experiment_report.md (if any)
              ```markdown
              ${phase3_report_md}
              ```

              ## 6) Phase 3 optimization history (if any)
              ```markdown
              ${phase3_optim_history_md}
              ```

              ## 7) Iteration tables (machine extracted)
              ### Phase 2 iterations
              ```json
              ${phase2_iters_json}
              ```

              ### Phase 3 iterations
              ```json
              ${phase3_iters_json}
              ```

              ## 8) Logs (head+tail excerpts)
              ### pipeline log excerpt
              ```text
              ${pipeline_log_excerpt}
              ```

              ### phase2 log excerpt
              ```text
              ${phase2_log_excerpt}
              ```

              ### phase3 log excerpt
              ```text
              ${phase3_log_excerpt}
              ```
            """
        )
        p.write_text(default_yaml, encoding="utf-8")
    except Exception as e:
        print(f"[WARN] Failed to create default final report prompt file: {e}")


def load_final_prompt() -> Tuple[str, str]:
    ensure_final_prompt_file_exists()
    if yaml is None:
        return ("You are a scientific report writer. Output Markdown only.", "${pipeline_summary_json}")
    try:
        data = yaml.safe_load(Path(FINAL_REPORT_PROMPT_PATH).read_text(encoding="utf-8")) or {}
        system = str(data.get("system") or "").strip()
        user_t = str(data.get("user_template") or "").strip()
        if not system or not user_t:
            raise ValueError("final_report.yaml missing required fields")
        return system, user_t
    except Exception as e:
        print(f"[WARN] Failed to load final_report.yaml ({FINAL_REPORT_PROMPT_PATH}): {e}")
        return ("You are a scientific report writer. Output Markdown only.", "${pipeline_summary_json}")


# =============================================================================
# LLM client (robust) - OpenAI compatible
# =============================================================================


def resolve_report_llm_cfg(pipe_cfg: Optional[Dict[str, Any]]) -> Dict[str, Any]:
    """Resolve the report-only LLM config.

    Precedence:
      1) pipeline_config.llm_report (preferred)
      2) pipeline_config.llm (fallback)
      3) env vars

    Also supports api_key_env.
    """

    base_llm_raw = (pipe_cfg or {}).get("llm") if isinstance((pipe_cfg or {}).get("llm"), dict) else {}
    report_llm_raw = (pipe_cfg or {}).get("llm_report") if isinstance((pipe_cfg or {}).get("llm_report"), dict) else {}

    base_llm = drop_none(base_llm_raw) if isinstance(base_llm_raw, dict) else {}
    report_llm = drop_none(report_llm_raw) if isinstance(report_llm_raw, dict) else {}
    llm = deep_merge(base_llm, report_llm) if report_llm else base_llm

    base_url = llm.get("base_url") or os.environ.get("OPENAI_BASE_URL") or "https://api.openai.com/v1"
    model = llm.get("model") or os.environ.get("OPENAI_MODEL") or "gpt-4o-mini"

    api_key = llm.get("api_key")
    if not api_key:
        api_key_env = llm.get("api_key_env") or "OPENAI_API_KEY"
        api_key = os.environ.get(api_key_env)

    temperature = llm.get("temperature")
    temperature = 0.2 if temperature is None else float(temperature)

    max_tokens = llm.get("max_tokens")
    max_tokens = 8192 if max_tokens is None else int(max_tokens)

    timeout = llm.get("timeout")
    timeout = 600 if timeout is None else int(timeout)

    return {
        "base_url": str(base_url).rstrip("/"),
        "model": model,
        "api_key": api_key,
        "temperature": temperature,
        "max_tokens": max_tokens,
        "timeout": timeout,
    }


def _post_request(url: str, headers: Dict[str, str], payload: Dict[str, Any], timeout: int, retries: int = 3) -> Dict[str, Any]:
    last_err: Optional[Exception] = None
    for i in range(retries + 1):
        try:
            r = requests.post(url, headers=headers, json=payload, timeout=timeout)
            if r.status_code == 200:
                return r.json()

            # retry on typical transient failures
            if r.status_code in {408, 409, 429, 500, 502, 503, 504}:
                time.sleep(1.0 + 0.8 * i)
                continue

            # non-retriable: include small excerpt
            txt = (r.text or "").strip()
            raise RuntimeError(f"LLM HTTP {r.status_code}: {txt[:800]}")
        except Exception as e:
            last_err = e
            time.sleep(1.0 + 0.8 * i)
    raise RuntimeError(f"LLM request failed after {retries} retries. Last error: {last_err}")


def chat_text(messages: List[Dict[str, Any]], llm_cfg: Dict[str, Any]) -> str:
    if not llm_cfg.get("api_key"):
        raise RuntimeError("Missing API key for report generation. Use llm_report.api_key_env or OPENAI_API_KEY.")

    url = llm_cfg["base_url"].rstrip("/") + "/chat/completions"
    headers = {
        "Authorization": f"Bearer {llm_cfg['api_key']}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": llm_cfg["model"],
        "messages": messages,
        "temperature": float(llm_cfg.get("temperature", 0.2)),
        "max_tokens": int(llm_cfg.get("max_tokens", 8192)),
        "stream": False,
    }

    data = _post_request(url, headers, payload, int(llm_cfg.get("timeout", 600)))
    try:
        content = data["choices"][0]["message"]["content"]
        return (content or "").strip()
    except Exception:
        return ""


# =============================================================================
# Selecting best runs + iteration extraction
# =============================================================================


def phase2_iters_detail_from_log(log_text: str, metric: str) -> List[Dict[str, Any]]:
    iters: Dict[int, Dict[str, Any]] = {}
    cur_iter: Optional[int] = None
    bug_markers = [
        "Notebook Generation Failed",
        "LLM_GEN_FAILURE",
        "CRITICAL PARSE FAILURE",
        "[GRAPH] ‚ùå",
        "Error in Node",
        "Initiating Adaptive Fix",
        "[FIX]",
        "Traceback",
        "Exception:",
    ]

    for ln in (log_text or "").splitlines():
        m = re.search(r"ITERATION\s+(\d+)/(\d+)", ln)
        if m:
            cur_iter = int(m.group(1))
            iters.setdefault(cur_iter, {"iter": cur_iter, "bug": False, "score": None, "notes": []})
            continue

        if cur_iter is None:
            continue

        rec = iters.setdefault(cur_iter, {"iter": cur_iter, "bug": False, "score": None, "notes": []})

        if any(x in ln for x in bug_markers):
            rec["bug"] = True

        mm = re.search(rf"\[CHECK\].*?\b{re.escape(metric)}\s*:\s*([-+]?\d*\.?\d+(?:[eE][-+]?\d+)?)", ln)
        if mm:
            try:
                rec["score"] = float(mm.group(1))
            except Exception:
                pass

        if "prompt_run_" in ln or "Saved" in ln or "SUCCESS" in ln or "FAIL" in ln:
            if len(rec["notes"]) < 6:
                rec["notes"].append(ln.strip())

    return [iters[k] for k in sorted(iters.keys())]


def phase3_iters_from_history_state(workspace_dir: str) -> List[Dict[str, Any]]:
    p = os.path.join(workspace_dir, "history_state.json")
    hist = safe_read_json(p)
    if not isinstance(hist, list):
        return []
    out = []
    for rec in hist:
        if not isinstance(rec, dict):
            continue
        out.append(
            {
                "iter": rec.get("iter"),
                "decision": rec.get("decision"),
                "focus": rec.get("focus"),
                "strategy": rec.get("strategy"),
                "status": rec.get("status"),
                "score": rec.get("score"),
                "reflection": rec.get("reflection"),
                "semantic_gradient": rec.get("semantic_gradient"),
                "tasks": rec.get("task_names") or rec.get("tasks"),
            }
        )
    return out


# =============================================================================
# Main entry: generate final report
# =============================================================================


def generate_final_report(
    *,
    ds_name: str,
    results_root: str,
    pipeline_summary: Dict[str, Any],
    stage1_cfg: Dict[str, Any],
    stage2_cfg: Dict[str, Any],
    stage3_cfg: Dict[str, Any],
    stage_timings: Dict[str, Dict[str, float]],
    phase_logs: Dict[str, str],
    pipeline_log_path: str,
    pipe_cfg: Optional[Dict[str, Any]],
    # Explicit path arguments to replace search functions
    explicit_p2_path: Optional[str] = None,
    explicit_p3_path: Optional[str] = None,
    direction: str,
    metric: str,
    # [NEW] Optional override for output directory (e.g., logs folder)
    output_base_dir: Optional[str] = None,
) -> Optional[str]:
    """Generate final_analysis_report.md and export best code to results/${dataset}/finall_results/."""

    # [MODIFIED] Determine output directory: use output_base_dir if provided, else results_root
    base_out = output_base_dir if output_base_dir else results_root
    final_dir = os.path.join(base_out, FINAL_REPORT_DIRNAME)
    os.makedirs(final_dir, exist_ok=True)

    # Locate key artifacts (reading always from results_root or explicit paths)
    phase1_summary_path = os.path.join(results_root, "design_analysis", "reference", "summary_report.md")

    # Use explicit paths instead of searching based on time
    best_p2_dir = explicit_p2_path
    if best_p2_dir and not os.path.exists(best_p2_dir):
        # Fallback only if provided path is bad (shouldn't happen with log tracker)
        print(f"[WARN] Explicit P2 path provided but not found: {best_p2_dir}")
        best_p2_dir = None
        
    phase2_report_path = os.path.join(best_p2_dir, "experiment_report.md") if best_p2_dir else ""

    p3_ws = explicit_p3_path
    if p3_ws and not os.path.exists(p3_ws):
        print(f"[WARN] Explicit P3 path provided but not found: {p3_ws}")
        p3_ws = None
        
    phase3_report_path = os.path.join(p3_ws, "experiment_report.md") if p3_ws else ""
    phase3_opt_hist_path = os.path.join(p3_ws, "optimization_history.md") if p3_ws else ""

    # Export best code
    best_nb_src = ""
    # Priority: Phase 3 Best -> Phase 2 Executed -> Phase 2 Prompt
    if p3_ws and os.path.exists(os.path.join(p3_ws, "notebook_best.ipynb")):
        best_nb_src = os.path.join(p3_ws, "notebook_best.ipynb")
    elif best_p2_dir and os.path.exists(os.path.join(best_p2_dir, "notebook_prompt_exec.ipynb")):
        best_nb_src = os.path.join(best_p2_dir, "notebook_prompt_exec.ipynb")
    elif best_p2_dir and os.path.exists(os.path.join(best_p2_dir, "notebook_prompt.ipynb")):
        best_nb_src = os.path.join(best_p2_dir, "notebook_prompt.ipynb")

    best_nb_dst = safe_copy(best_nb_src, final_dir, "best_code.ipynb") if best_nb_src else None
    if best_nb_dst:
        export_notebook_as_py(best_nb_dst, os.path.join(final_dir, "best_code.py"))

    # Also copy key inputs for convenience
    safe_copy(phase1_summary_path, final_dir, "phase1_summary_report.md")
    
    if best_p2_dir:
        safe_copy(os.path.join(best_p2_dir, "metrics.json"), final_dir, "phase2_best_metrics.json")
        safe_copy(phase2_report_path, final_dir, "phase2_best_experiment_report.md")
        
    if p3_ws:
        safe_copy(os.path.join(p3_ws, "metrics_best.json"), final_dir, "phase3_best_metrics.json")
        safe_copy(phase3_report_path, final_dir, "phase3_experiment_report.md")
        safe_copy(phase3_opt_hist_path, final_dir, "phase3_optimization_history.md")
        safe_copy(os.path.join(p3_ws, "history_state.json"), final_dir, "phase3_history_state.json")
        
    safe_copy(pipeline_log_path, final_dir, os.path.basename(pipeline_log_path))
    for _k, lp in (phase_logs or {}).items():
        safe_copy(lp, final_dir, os.path.basename(lp))

    # Build iteration tables
    p2_log_text = read_text(phase_logs.get("Phase 2", ""))
    p2_iters = phase2_iters_detail_from_log(p2_log_text, metric)
    p3_iters = phase3_iters_from_history_state(p3_ws) if p3_ws else []

    system_prompt, user_template = load_final_prompt()
    llm_cfg = resolve_report_llm_cfg(pipe_cfg)

    pipeline_summary_json = json.dumps(redact_secrets(pipeline_summary), ensure_ascii=False, indent=2)
    p1_cfg_json = json.dumps(redact_secrets(stage1_cfg), ensure_ascii=False, indent=2)
    p2_cfg_json = json.dumps(redact_secrets(stage2_cfg), ensure_ascii=False, indent=2)
    p3_cfg_json = json.dumps(redact_secrets(stage3_cfg), ensure_ascii=False, indent=2)

    # progressive shrinking to avoid context explosions
    last_err: Optional[str] = None
    for shrink in [1.0, 0.7, 0.5, 0.35]:
        md_cap = int(80_000 * shrink)
        log_cap = int(140_000 * shrink)
        try:
            vars = {
                "dataset_name": ds_name,
                "final_output_dir": final_dir,
                "pipeline_summary_json": pipeline_summary_json,
                "phase1_config_json": p1_cfg_json,
                "phase2_config_json": p2_cfg_json,
                "phase3_config_json": p3_cfg_json,
                "phase1_summary_md": read_text_limited(phase1_summary_path, max_chars=md_cap),
                "phase2_report_md": read_text_limited(phase2_report_path, max_chars=md_cap) if phase2_report_path else "",
                "phase3_report_md": read_text_limited(phase3_report_path, max_chars=md_cap) if phase3_report_path else "",
                "phase3_optim_history_md": read_text_limited(phase3_opt_hist_path, max_chars=md_cap) if phase3_opt_hist_path else "",
                "phase2_iters_json": json.dumps(p2_iters, ensure_ascii=False, indent=2),
                "phase3_iters_json": json.dumps(p3_iters, ensure_ascii=False, indent=2),
                "pipeline_log_excerpt": read_head_tail_lines(pipeline_log_path, max_chars=log_cap),
                "phase2_log_excerpt": read_head_tail_lines(phase_logs.get("Phase 2", ""), max_chars=log_cap),
                "phase3_log_excerpt": read_head_tail_lines(phase_logs.get("Phase 3", ""), max_chars=log_cap),
            }

            user_prompt = user_template
            for k, v in vars.items():
                user_prompt = user_prompt.replace("${" + k + "}", str(v))

            report_md = chat_text(
                [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                llm_cfg,
            )

            if report_md:
                out_path = os.path.join(final_dir, "final_analysis_report.md")
                with open(out_path, "w", encoding="utf-8") as f:
                    f.write(report_md)
                return out_path
        except Exception as e:
            last_err = str(e)
            # If likely context error, continue shrinking; otherwise still retry smaller once.
            continue

    if last_err:
        print(f"[WARN] Final report generation failed: {last_err}")
    return None